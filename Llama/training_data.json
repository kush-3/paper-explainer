[
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain gradients in simple terms<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nA gradient tells the model which direction to adjust its weights to reduce error. It's like checking which way the slope goes on a hill so you know whether to step left or right.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWhat are weights in a neural network?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nWeights are numbers that decide how important each input is. During training, the model keeps adjusting these weights to improve predictions.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain bias in neural networks simply<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nA bias is a number added to a neuron's output that helps the model shift the activation curve. Think of it like adjusting the baseline before applying a rule.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWhat is normalization?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nNormalization scales numbers to a smaller, consistent range so the model can learn more smoothly. Large differences in input values can slow learning down.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain supervised learning simply<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nSupervised learning is when the model is trained on examples that already have answers. It\u2019s like studying from a solved textbook.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain unsupervised learning simply<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nUnsupervised learning has no answers in the data. The model tries to find hidden patterns by itself, like grouping similar things together.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWhat are logits?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nLogits are the raw output numbers from a neural network before converting them into probabilities. Softmax turns them into values between 0 and 1.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain why activation functions matter<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nActivation functions help the network learn complex patterns by adding non-linearity. Without them, the whole network would behave like one big linear equation.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWhat is the purpose of a hidden layer?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nHidden layers transform inputs into useful representations. They discover patterns that aren't obvious in the raw data.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain cross entropy simply<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nCross entropy measures how different the model\u2019s predicted probability distribution is from the real one. Lower cross entropy means the model is predicting better.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain why deep networks need a lot of data<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nDeep networks have many parameters to learn. Without enough data, they memorize small details instead of learning real patterns.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWhat is a learning schedule?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nA learning schedule changes the learning rate during training. It usually starts high and gets smaller over time to help the model fine-tune its understanding.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain what zero-shot learning is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nZero-shot learning is when a model answers a question or performs a task it was never directly trained on. It relies on patterns learned from other tasks.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWhat is a model checkpoint?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nA checkpoint is a saved version of the model during training. If something breaks, you can reload the checkpoint instead of starting from zero.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain early stopping in simple words<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nEarly stopping ends training when the model stops improving on validation data. This prevents overfitting by not training for too long.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWhat is an encoder in AI models?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nAn encoder turns input data into a compressed representation. It captures the important features in a smaller form that other parts of the model can use.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain perplexity simply<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nPerplexity measures how confused a language model is. Lower perplexity means the model is better at predicting the next word.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain vector databases simply<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nA vector database stores embeddings\u2014special numbers that represent meaning. It helps find items that are most similar in meaning to a query.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWhy do LLMs need attention layers?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nAttention layers let the model focus on the most relevant words in a sentence. This helps it understand context better than old models that read one word at a time.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain token limits simply<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nA token limit is the maximum amount of text a model can read at once. If you exceed it, the model can\u2019t process all the input, just like trying to stuff too many papers into a small folder.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain neural networks using a pizza analogy<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nThink of each layer as a chef adding toppings. The first layer adds basic toppings, the next layer adds more flavors, and by the last layer you have a full pizza. Training is adjusting how much of each topping each chef adds.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nDescribe overfitting like you're talking to a friend<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nBro, overfitting is when your model becomes that one kid who memorizes every answer from last year\u2019s test but fails completely when the teacher changes the questions. No general skills. Just vibes.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain backpropagation like it's gossip<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nBackprop is like drama spreading backwards through a friend group. The final mistake is the big rumor, and each layer passes blame to the one before it until everyone adjusts their behavior.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain transformers like you're hyping them up<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nTransformers are the superheroes of AI. They don\u2019t read text in order\u2014they look at the whole thing at once like they have super-vision. That\u2019s why they\u2019re insane at language tasks.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain loss functions using a gym analogy<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nLoss is like tracking your personal records at the gym. High loss means you're lifting poorly. Lower loss means you're getting stronger. Training is trying to beat yesterday\u2019s score every day.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain neural networks like I'm 5<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nImagine a bunch of tiny helpers passing notes to each other. The first helper looks at a picture, writes something down, and passes it to the next helper. Each helper adds their own notes. By the end, the last helper guesses what the picture is!<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWhat is machine learning like I'm 5?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nIt's like teaching a dog tricks. You show the dog what to do, give it treats when it's right, and eventually it learns. Computers learn the same way - we show them examples and tell them yes or no until they figure it out.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain loss function like I'm 5<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nIt's a score that tells the computer how wrong it was. Like when you guess someone's age - if you guess 10 and they're 12, you were off by 2. The computer tries to make that number as tiny as possible.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWhat are embeddings like I'm 5?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nWords are turned into secret number codes. Similar words get similar codes. So 'happy' and 'joyful' have codes that are close together, but 'happy' and 'refrigerator' have codes that are far apart.<|eot_id|>"
  },
  {
    "text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nExplain training a model like I'm 5<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nIt's like practicing basketball. You shoot, miss, and adjust. Shoot again, get closer, adjust again. After thousands of shots, you start making baskets. The model does the same thing with math problems.<|eot_id|>"
  }
]